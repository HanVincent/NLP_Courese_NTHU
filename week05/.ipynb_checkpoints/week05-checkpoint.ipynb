{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.corpus import wordnet as wn\n",
    "import re, random, nltk, json\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import DictionaryProbDist as D\n",
    "\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def wnTag(pos): return {'noun': 'n', 'verb': 'v', 'adjective': 'a', 'adverb': 'r'}[pos]\n",
    "\n",
    "def split_col(data):\n",
    "    training = [ line.strip().split('\\t') for line in data if line.strip() != '' ]\n",
    "    return training\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def isHead(head, word, tag):\n",
    "    try:\n",
    "        return lmtzr.lemmatize(word, tag) == head\n",
    "    except:\n",
    "        return False\n",
    "        \n",
    "TF = defaultdict(lambda: Counter())\n",
    "DF = defaultdict(lambda: [])\n",
    "\n",
    "def set_TF_DF(training):   \n",
    "    for wnid, wncat, senseDef, target in training:\n",
    "        head, pos = wnid.split('-')[:2]\n",
    "        for word in words(senseDef):\n",
    "            if word != head and not isHead(head, word, pos):\n",
    "                TF[word][wncat] += 1\n",
    "                DF[word] += [] if wncat in DF[word] else [wncat]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('wn.in.evp.cat.txt', 'r').readlines()[:]\n",
    "random.shuffle(data)\n",
    "data = split_col(data)\n",
    "split_point = len(data)*9//10\n",
    "train_set, test_set = data[:split_point], data[split_point:]\n",
    "set_TF_DF(train_set)\n",
    "\n",
    "nTF = sorted(TF, key=lambda k: sum(TF[k].values()),reverse=True)[:200]\n",
    "nDF = sorted(DF, key=lambda k: len(DF[k]),reverse=True)[:200]\n",
    "stopwords = set(nTF).intersection(set(nDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['burden-n-4', 'idea.n.01', 'burden||the central idea that is expanded in a document or discourse||', \"{'burden-n-1': 'cognition.n.01', 'burden-n-2': 'artifact.n.01', 'burden-n-3': 'message.n.02', 'burden-n-4': 'idea.n.01'}\"]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(wnid, wncat, senseDef, target):\n",
    "    head, pos = wnid.split('-')[:2]\n",
    "    features = {'pos': pos}\n",
    "    for word in words(senseDef):\n",
    "        if word in stopwords: continue\n",
    "\n",
    "        if word != head and not isHead(head, word, pos): # 有需要過濾自己嗎？\n",
    "            for cat, count in TF[word].most_common(10):\n",
    "                features.update({cat: count * 1000/len(DF[word])}) # tf*int(1000/df)\n",
    "    return (features, wncat)\n",
    "    \n",
    "    \n",
    "def LG_gender(train_set):\n",
    "    print('== SkLearn MaxEnt ==')\n",
    "    \n",
    "    from nltk.classify import SklearnClassifier \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    sklearn_classifier = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set)\n",
    "    return sklearn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'act.n.02': 250.0,\n",
      "  'activity.n.01': 125.0,\n",
      "  'adv.all': 127.65957446808511,\n",
      "  'agency.n.01': 103.44827586206897,\n",
      "  'area.n.01': 103.44827586206897,\n",
      "  'become.v.03': 106.38297872340425,\n",
      "  'change.v.02': 127.65957446808511,\n",
      "  'communicate.v.02': 125.0,\n",
      "  'communication.n.02': 125.0,\n",
      "  'content.n.05': 250.0,\n",
      "  'convey.v.01': 375.0,\n",
      "  'currency.n.01': 68.96551724137932,\n",
      "  'declare.v.01': 85.1063829787234,\n",
      "  'expressive_style.n.01': 125.0,\n",
      "  'find.v.01': 106.38297872340425,\n",
      "  'hash_out.v.01': 125.0,\n",
      "  'idea.n.01': 125.0,\n",
      "  'important.a.01': 137.93103448275863,\n",
      "  'legal_document.n.01': 562.5,\n",
      "  'make.v.03': 62.5,\n",
      "  'measure.n.02': 250.0,\n",
      "  'message.n.02': 62.5,\n",
      "  'move.v.02': 125.0,\n",
      "  'music.n.01': 62.5,\n",
      "  'object.n.01': 137.93103448275863,\n",
      "  'pos': 'n',\n",
      "  'position.n.06': 250.0,\n",
      "  'relation.n.01': 125.0,\n",
      "  'state.n.02': 62.5,\n",
      "  'time_period.n.01': 250.0,\n",
      "  'understand.v.02': 127.65957446808511,\n",
      "  'writing.n.02': 625.0},\n",
      " 'idea.n.01')\n"
     ]
    }
   ],
   "source": [
    "pprint(gender_features(*train_set[0])) # 看 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SkLearn MaxEnt ==\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_featuresets = [ gender_features(*x) for x in train_set ]\n",
    "test_featuresets = [ gender_features(*x) for x in test_set ]\n",
    "sklearn_classifier = LG_gender(train_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857758620689655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.621551724137931\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "print(nltk.classify.accuracy(sklearn_classifier, test_featuresets)) # 未過濾\n",
    "    \n",
    "correct = 0\n",
    "for i, (feature, label) in enumerate(test_featuresets):\n",
    "    prob_dict = sklearn_classifier.prob_classify(test_featuresets[i][0])._prob_dict\n",
    "\n",
    "    targets = json.loads(test_set[i][3].replace(\"'\", '\"')).values()\n",
    "    probs = [(target, prob_dict[target]) for target in targets if target in prob_dict]\n",
    "    if not probs:\n",
    "        if max(prob_dict.items(), key=lambda x: x[1])[0] == label:\n",
    "            correct+=1\n",
    "    else: \n",
    "        if max(probs, key=lambda x: x[1])[0] == label:\n",
    "            correct+=1\n",
    "print(correct/len(test_featuresets))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     data = open('evp.in.wn.cat.txt', 'r').readlines()\n",
    "#     testing = split_col(data)\n",
    "#     test_featuresets = [ gender_features(*x) for x in testing ]\n",
    "    \n",
    "#     fs = open('result.txt', 'w', encoding='utf8')\n",
    "\n",
    "#     for i, (feature, label) in enumerate(test_featuresets):\n",
    "#         prob_dict = sklearn_classifier.prob_classify(test_featuresets[i][0])._prob_dict\n",
    "\n",
    "#         targets = json.loads(testing[i][3].replace(\"'\", '\"')).values()\n",
    "#         probs = [(target, prob_dict[target]) for target in targets if target in prob_dict]\n",
    "#         if not probs:\n",
    "#             print()\n",
    "#         else: \n",
    "\n",
    "#     fs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
