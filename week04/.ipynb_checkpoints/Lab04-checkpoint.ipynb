{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "count = dict()\n",
    "count_c = defaultdict(lambda: 0)\n",
    "for line in open('count_1edit.txt', 'r', encoding='utf8'):\n",
    "    wc, num = line.strip().split('\\t')\n",
    "    w, c = wc.split('|')\n",
    "    count[(w, c)] = int(num)\n",
    "    count_c[c] += int(num)\n",
    "Ncount = Counter(count.values())\n",
    "\n",
    "Nall = len(count.keys())\n",
    "N0 = 26*26*26*26+2*26*26*26+26*26 - Nall\n",
    "Nr = [ N0 if r == 0 else Ncount[r] for r in range(12) ]\n",
    "\n",
    "def smooth(count, r=10):\n",
    "    if count <= r:\n",
    "        return (count+1)*Nr[count+1] / Nr[count]\n",
    "    else:\n",
    "        return count\n",
    "\n",
    "def Pedit(w, c):\n",
    "    if (w, c) not in count and count_c[c] > 0:\n",
    "        return smooth(0) / count_c[c]\n",
    "    if count_c[c] > 0:\n",
    "        return smooth(count[(w, c)]) / count_c[c]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "# WORDS = Counter(open('big.txt').read().split())\n",
    "\n",
    "def Pw(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    states = [ ('', word, 0, Pw(word), 1) ]\n",
    "    for i in range(len(word)):\n",
    "        # print(i, states[:3])\n",
    "        STATES = [ s for state in states for s in next_states(state) ]\n",
    "        states = sorted(STATES, key=lambda x: x[2])\n",
    "\n",
    "        unique, new_states = set(), []\n",
    "        for state in states:\n",
    "            if state[0] + state[1] in unique: continue\n",
    "\n",
    "            unique.add(state[0] + state[1])\n",
    "            new_states.append(state)\n",
    "        states = new_states\n",
    "        states = sorted(states, key=lambda x: P(x[3], x[4]), reverse=True) [:500]# [:MAXBEAM]\n",
    "    return states[:10]\n",
    "\n",
    "def next_states(state):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    L, R, edit, prob, ped = state\n",
    "    R0, R1 = R[0], R[1:]\n",
    "    if edit == 2: return [( L + R0, R1, edit, prob, ped*0.8 )]\n",
    "    noedit    = [( L + R0, R1, edit, prob, ped*0.8 )]\n",
    "    delete    = [( L, R1, edit+1, Pw(L + R1), ped * Pedit(L[-1]+R0, L[-1]))]  if len(L) > 0 else []\n",
    "    insert    = [( L + R0 + c, R1, edit+1, Pw(L + R0 + c + R1), ped * Pedit(R0, R0 + c) ) for c in letters]\n",
    "    replace   = [( L + c, R1, edit+1, Pw(L + c + R1), ped * Pedit(R0, c) ) for c in letters]\n",
    "    transpose = [( L[:-1] + R0 + L[-1], R1, edit+1, Pw(L[:-1] + R0 + L[-1] + R1), ped * Pedit(L[-1]+R0, R0+L[-1]) )] if len(L) > 1 else []\n",
    "    return set(noedit + delete + replace + insert + transpose)\n",
    "\n",
    "'''Combining channel probability with word probability to score states'''\n",
    "def P(pw, pedit):\n",
    "    return pw*pedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"http://api.netspeak.org/netspeak3/search?query=%s\"\n",
    "# http://api.netspeak.org/netspeak3/search?query=i%20felt%20very&format=json&callback=__gwt_jsonp__.P20.onSuccess\n",
    "class NetSpeak:\n",
    "    def __init__(self):\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "        self.page = None\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def __getPageContent(self, url):\n",
    "        return requests.get(url, headers=self.headers).text\n",
    "        # return self.opener.open(url).read()\n",
    "\n",
    "    def __rolling(self, url, maxfreq=None):\n",
    "        if maxfreq:\n",
    "            webdata = self.__getPageContent(url + \"&maxfreq=%s\" % maxfreq)\n",
    "        else:\n",
    "            webdata = self.__getPageContent(url)\n",
    "        if webdata:\n",
    "            # webdata = webdata.decode('utf-8')\n",
    "            results = [data.split('\\t') for data in webdata.splitlines()]\n",
    "            results = [(data[2], float(data[1])) for data in results]\n",
    "            lastFreq = int(results[-1][1])\n",
    "            if lastFreq != maxfreq:\n",
    "                return results + self.__rolling(url, lastFreq)\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def search(self, query):\n",
    "        if query in self.dictionary: return self.dictionary[query]\n",
    "        \n",
    "        queries = query.lower().split()\n",
    "        new_query = []\n",
    "        for token in queries:\n",
    "            if token.count('|') > 0:\n",
    "                new_query.append('[+{0}+]'.format('+'.join(token.split('|'))))\n",
    "            elif token == '*':\n",
    "                new_query.append('?')\n",
    "            else:\n",
    "                new_query.append(token)\n",
    "        new_query = '+'.join(new_query)\n",
    "        url = API_URL % (new_query.replace(' ', '+'))\n",
    "        self.dictionary[query] = self.__rolling(url)\n",
    "        return self.dictionary[query]\n",
    "    \n",
    "SE = NetSpeak() # singleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusable = dict([line.strip().split('\\t') for line in open('lab4.confusables.txt', 'r', encoding='utf8')])\n",
    "\n",
    "def get_trigrams(tokens):\n",
    "    return [tokens[i:i+3] for i in range(len(tokens) - 2)]\n",
    "\n",
    "def get_lowest_tri(tokens):\n",
    "    trigrams, pairs = get_trigrams(tokens), [] # (index, count, trigram)\n",
    "    for i, tri in enumerate(trigrams):\n",
    "        res = SE.search(' '.join(tri))\n",
    "        if res:\n",
    "            pairs.append((i, res[0][1], tri))\n",
    "        else:\n",
    "            pairs.append((i, 0, tri))\n",
    "\n",
    "    minimum = min(pairs, key=lambda x: x[1])[1]\n",
    "    pairs = [p for p in pairs if p[1] == minimum]\n",
    "    \n",
    "    lowest_pair = pairs[0]\n",
    "    lowest_start = lowest_pair[0]\n",
    "    \n",
    "    # 可能有多個 trigram count = 0，那就找 trigram 內的單字有錯字的（或機率最小的）\n",
    "    if len(pairs) > 1:\n",
    "        min_prob = math.inf\n",
    "        for pair in pairs:\n",
    "            prob = 1.0\n",
    "            for word in pair[2]:\n",
    "                prob *= 1 if word.istitle() else Pw(word)\n",
    "                \n",
    "            if prob < min_prob:\n",
    "                min_prob = prob\n",
    "                lowest_pair = pair\n",
    "                lowest_start = lowest_pair[0]\n",
    "        \n",
    "    return lowest_pair, lowest_start\n",
    "\n",
    "def get_max_sent(tokens, lowest_start):\n",
    "    # (sent, error_word, correct_token, can_tokens, count)\n",
    "    best = (None, None, None, None, -math.inf)\n",
    "    \n",
    "    for i in range(lowest_start, lowest_start + 3):\n",
    "        can_tokens = [can[0] for can in correction(tokens[i])] + ([confusable[tokens[i]]] if tokens[i] in confusable else [])\n",
    "        for c in can_tokens: # get correction candidates (a word)\n",
    "            count = 1.0\n",
    "            sent = tokens[:i] + [c] + tokens[i+1:]\n",
    "            trigrams = get_trigrams(sent)\n",
    "        \n",
    "            for tri in trigrams:\n",
    "                res = SE.search(' '.join(tri))\n",
    "                count *= res[0][1] if res else 0\n",
    "\n",
    "            best = (sent, tokens[i], c, can_tokens, count) if count > best[-1] else best\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['make', 'a', 'deep', 'hole'],\n",
       " 'depe',\n",
       " 'deep',\n",
       " ['deep',\n",
       "  'type',\n",
       "  'keep',\n",
       "  'depth',\n",
       "  'were',\n",
       "  'dip',\n",
       "  'weep',\n",
       "  'debt',\n",
       "  'pipe',\n",
       "  'kept'],\n",
       " 337358304.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_max_sent('make a depe hole'.split(' '), 1)\n",
    "# get_lowest_tri('they kill birds with their nerrow \tthey kill birds with their arrow'.split('\\t')[0].strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Error: depe\n",
      "Candidates: ['deep', 'type', 'keep', 'depth', 'were', 'dip', 'weep', 'debt', 'pipe', 'kept']\n",
      "Correction: deep\n",
      "make a depe hole -> make a deep hole (correct: make a deep hole )\n",
      "hits = 1\n",
      "\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "lines = ['make a depe hole \tmake a deep hole', 'they kill birds with their nerrow \tthey kill birds with their arrow', 'Mr J. was very angray \tMr J. was very angry']\n",
    "# lines = ['Mr J. was very angray \tMr J. was very angry']\n",
    "cor, hits = 0, 0\n",
    "\n",
    "for line in open('lab4.test.1.txt', 'r', encoding='utf8').readlines()[:20]:\n",
    "# for line in lines:\n",
    "    print(\"====================================\")\n",
    "    wrong, right = line.split('\\t')\n",
    "\n",
    "    tokens = wrong.strip().split(' ') # words(open('big.txt').read())) # or using regex\n",
    "    lowest_pair, lowest_pos = get_lowest_tri(tokens)\n",
    "    \n",
    "    sent, error_word, right_word, candidates, _ = get_max_sent(tokens, lowest_pos)\n",
    "    sent, wrong, right = ' '.join(sent).strip(), wrong.strip(), right.strip()\n",
    "    \n",
    "    if sent == right: hits += 1\n",
    "    cor += 1\n",
    "    \n",
    "    print(\"Error:\", error_word)\n",
    "    print(\"Candidates:\", candidates)\n",
    "    print(\"Correction:\", right_word)\n",
    "    print(wrong, \"->\", sent, \"(correct:\", right, \")\")\n",
    "    print(\"hits =\", hits)\n",
    "    print()\n",
    "\n",
    "print(\"Precision:\", hits/cor)\n",
    "print(\"FalseAlarm:\", (cor-hits)/cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
