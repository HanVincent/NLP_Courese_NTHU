{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "count = dict()\n",
    "count_c = defaultdict(lambda: 0)\n",
    "for line in open('count_1edit.txt', 'r', encoding='utf8'):\n",
    "    wc, num = line.strip().split('\\t')\n",
    "    w, c = wc.split('|')\n",
    "    count[(w, c)] = int(num)\n",
    "    count_c[c] += int(num)\n",
    "Ncount = Counter(count.values())\n",
    "\n",
    "Nall = len(count.keys())\n",
    "N0 = 26*26*26*26+2*26*26*26+26*26 - Nall\n",
    "Nr = [ N0 if r == 0 else Ncount[r] for r in range(12) ]\n",
    "\n",
    "def smooth(count, r=10):\n",
    "    if count <= r:\n",
    "        return (count+1)*Nr[count+1] / Nr[count]\n",
    "    else:\n",
    "        return count\n",
    "\n",
    "def Pedit(w, c):\n",
    "    if (w, c) not in count and count_c[c] > 0:\n",
    "        return smooth(0) / count_c[c]\n",
    "    if count_c[c] > 0:\n",
    "        return smooth(count[(w, c)]) / count_c[c]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "# WORDS = Counter(open('big.txt').read().split())\n",
    "\n",
    "def Pw(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    states = [ ('', word, 0, Pw(word), 1) ]\n",
    "    for i in range(len(word)):\n",
    "        # print(i, states[:3])\n",
    "        STATES = [ s for state in states for s in next_states(state) ]\n",
    "        states = sorted(STATES, key=lambda x: x[2])\n",
    "\n",
    "        unique, new_states = set(), []\n",
    "        for state in states:\n",
    "            if state[0] + state[1] in unique: continue\n",
    "\n",
    "            unique.add(state[0] + state[1])\n",
    "            new_states.append(state)\n",
    "        states = new_states\n",
    "        states = sorted(states, key=lambda x: P(x[3], x[4]), reverse=True) [:500]# [:MAXBEAM]\n",
    "    return states[:10]\n",
    "\n",
    "def next_states(state):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    L, R, edit, prob, ped = state\n",
    "    R0, R1 = R[0], R[1:]\n",
    "    if edit == 2: return [( L + R0, R1, edit, prob, ped*0.8 )]\n",
    "    noedit    = [( L + R0, R1, edit, prob, ped*0.8 )]\n",
    "    delete    = [( L, R1, edit+1, Pw(L + R1), ped * Pedit(L[-1]+R0, L[-1]))]  if len(L) > 0 else []\n",
    "    insert    = [( L + R0 + c, R1, edit+1, Pw(L + R0 + c + R1), ped * Pedit(R0, R0 + c) ) for c in letters]\n",
    "    replace   = [( L + c, R1, edit+1, Pw(L + c + R1), ped * Pedit(R0, c) ) for c in letters]\n",
    "    transpose = [( L[:-1] + R0 + L[-1], R1, edit+1, Pw(L[:-1] + R0 + L[-1] + R1), ped * Pedit(L[-1]+R0, R0+L[-1]) )] if len(L) > 1 else []\n",
    "    return set(noedit + delete + replace + insert + transpose)\n",
    "\n",
    "'''Combining channel probability with word probability to score states'''\n",
    "def P(pw, pedit):\n",
    "    return pw*pedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"http://api.netspeak.org/netspeak3/search?query=%s\"\n",
    "\n",
    "class NetSpeak:\n",
    "    def __init__(self):\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "        self.page = None\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def __getPageContent(self, url):\n",
    "        return requests.get(url, headers=self.headers).text\n",
    "        # return self.opener.open(url).read()\n",
    "\n",
    "    def __rolling(self, url, maxfreq=None):\n",
    "        if maxfreq:\n",
    "            webdata = self.__getPageContent(url + \"&maxfreq=%s\" % maxfreq)\n",
    "        else:\n",
    "            webdata = self.__getPageContent(url)\n",
    "        if webdata:\n",
    "            # webdata = webdata.decode('utf-8')\n",
    "            results = [data.split('\\t') for data in webdata.splitlines()]\n",
    "            results = [(data[2], float(data[1])) for data in results]\n",
    "            lastFreq = int(results[-1][1])\n",
    "            if lastFreq != maxfreq:\n",
    "                return results + self.__rolling(url, lastFreq)\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def search(self, query):\n",
    "        if query in self.dictionary: return self.dictionary[query]\n",
    "        \n",
    "        queries = query.lower().split()\n",
    "        new_query = []\n",
    "        for token in queries:\n",
    "            if token.count('|') > 0:\n",
    "                new_query.append('[+{0}+]'.format('+'.join(token.split('|'))))\n",
    "            elif token == '*':\n",
    "                new_query.append('?')\n",
    "            else:\n",
    "                new_query.append(token)\n",
    "        new_query = '+'.join(new_query)\n",
    "        url = API_URL % (new_query.replace(' ', '+'))\n",
    "        self.dictionary[query] = self.__rolling(url)\n",
    "        return self.dictionary[query]\n",
    "    \n",
    "SE = NetSpeak() # singleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusable = dict([line.strip().split('\\t') for line in open('lab4.confusables.txt', 'r', encoding='utf8')])\n",
    "\n",
    "def get_trigrams(tokens):\n",
    "    if len(tokens) < 3:\n",
    "        return [tokens]\n",
    "\n",
    "    return [tokens[i:i+3] for i in range(len(tokens) - 2)]\n",
    "\n",
    "def get_lowest_tri(tokens):\n",
    "    trigrams, pairs = get_trigrams(tokens), [] # (index, count, trigram)\n",
    "    for i, tri in enumerate(trigrams):\n",
    "        res = SE.search(' '.join(tri))\n",
    "        if res:\n",
    "            pairs.append((i, res[0][1], tri))\n",
    "        else:\n",
    "            pairs.append((i, 0, tri))\n",
    "\n",
    "    minimum = min(pairs, key=lambda x: x[1])[1]\n",
    "    pairs = [p for p in pairs if p[1] == minimum]\n",
    "    \n",
    "    lowest_pair = pairs[0]\n",
    "    lowest_start = lowest_pair[0]\n",
    "    \n",
    "    return lowest_pair, lowest_start\n",
    "\n",
    "def get_max_sent(tokens, lowest_start):\n",
    "    # (sent, error_word, correct_token, can_tokens, count)\n",
    "    best = (None, None, None, None, -math.inf)\n",
    "    \n",
    "    for i in range(lowest_start, lowest_start + min(3, len(tokens))):\n",
    "        can_tokens = [can[0] for can in correction(tokens[i])] + ([confusable[tokens[i]]] if tokens[i] in confusable else [])\n",
    "        for c in can_tokens: # get correction candidates (a word)\n",
    "            count = 1.0\n",
    "            sent = tokens[:i] + [c] + tokens[i+1:]\n",
    "            trigrams = get_trigrams(sent)\n",
    "        \n",
    "            for tri in trigrams:\n",
    "                res = SE.search(' '.join(tri))\n",
    "                count *= res[0][1] if res else 0\n",
    "\n",
    "            best = (sent, tokens[i], c, can_tokens, count) if count > best[-1] else best\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lines = ['I was on an exclation \tI was on an escalator', 'to tidy up his gardon \tto tidy up his garden','talk to the manger \ttalk to the manager', 'through the fance \tthrough the fence']\n",
    "\n",
    "cor, hits = 0, 0\n",
    "lines = open('lab4.test.1.txt', 'r', encoding='utf8').readlines()[:20]\n",
    "result = open('result.txt', 'w', encoding='utf8')\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "# for line in lines:\n",
    "    print(\"================\", i+1, \"===================\", file=result)\n",
    "    wrong, right = line.split('\\t')\n",
    "\n",
    "    tokens = wrong.strip().split(' ') # words(open('big.txt').read())) # or using regex\n",
    "    lowest_pair, lowest_pos = get_lowest_tri(tokens)\n",
    "    \n",
    "    sent, error_word, right_word, candidates, _ = get_max_sent(tokens, lowest_pos)\n",
    "    sent, wrong, right = ' '.join(sent).strip(), wrong.strip(), right.strip()\n",
    "    \n",
    "    if sent == right: hits += 1\n",
    "    cor += 1\n",
    "    \n",
    "    print(\"Error:\", error_word, file=result)\n",
    "    print(\"Candidates:\", candidates, file=result)\n",
    "    print(\"Correction:\", right_word, file=result)\n",
    "    print(wrong, \"->\", sent, \"(correct:\", right, \")\", file=result)\n",
    "    print(\"hits =\", hits, file=result)\n",
    "    print('\\n', file=result)\n",
    "\n",
    "print(\"Precision:\", hits/cor, file=result)\n",
    "print(\"FalseAlarm:\", (cor-hits)/cor, file=result)\n",
    "\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ 1 ===================\n",
      "[(0, 129142.0, ['I', 'felt', 'very']), (1, 0, ['felt', 'very', 'strang'])]\n",
      "================ 2 ===================\n",
      "[(0, 0, ['at', 'brake', 'time'])]\n",
      "================ 3 ===================\n",
      "[(0, 0, ['when', 'the', 'brack']), (1, 0, ['the', 'brack', 'was']), (2, 0, ['brack', 'was', 'finished'])]\n",
      "================ 4 ===================\n",
      "[(0, 0, ['in', 'the', 'weanter']), (1, 0, ['the', 'weanter', 'when']), (2, 0, ['weanter', 'when', 'it']), (3, 3293729.0, ['when', 'it', 'was']), (4, 42041.0, ['it', 'was', 'snowing'])]\n",
      "================ 5 ===================\n",
      "[(0, 3035254.0, ['I', 'thought', 'it']), (1, 3179068.0, ['thought', 'it', 'was']), (2, 18360734.0, ['it', 'was', 'a']), (3, 0, ['was', 'a', 'gost'])]\n",
      "================ 6 ===================\n",
      "[(0, 110.0, ['everything', 'expect', 'the']), (1, 49.0, ['expect', 'the', 'houses'])]\n",
      "================ 7 ===================\n",
      "[(0, 1767739.0, ['when', 'I', 'first']), (1, 0, ['I', 'first', 'steped'])]\n",
      "================ 8 ===================\n",
      "[(0, 1316801.0, ['I', 'was', 'on']), (1, 68060.0, ['was', 'on', 'an']), (2, 0, ['on', 'an', 'exclation'])]\n",
      "================ 9 ===================\n",
      "[(0, 0, ['I', 'noicey', 'that']), (1, 0, ['noicey', 'that', 'I']), (2, 5145758.0, ['that', 'I', 'was']), (3, 1316801.0, ['I', 'was', 'on']), (4, 98984.0, ['was', 'on', 'this']), (5, 102743.0, ['on', 'this', 'thing'])]\n",
      "================ 10 ===================\n",
      "[(0, 0, ['through', 'the', 'fance'])]\n",
      "================ 11 ===================\n",
      "[(0, 0, ['the', 'hunters', 'kille']), (1, 0, ['hunters', 'kille', 'them'])]\n",
      "================ 12 ===================\n",
      "[(0, 283.0, ['they', 'kill', 'birds']), (1, 116.0, ['kill', 'birds', 'with']), (2, 3423.0, ['birds', 'with', 'their']), (3, 0, ['with', 'their', 'nerrow'])]\n",
      "================ 13 ===================\n",
      "[(0, 0, ['make', 'a', 'depe']), (1, 0, ['a', 'depe', 'hole'])]\n",
      "================ 14 ===================\n",
      "[(0, 46540.0, ['to', 'tidy', 'up']), (1, 867.0, ['tidy', 'up', 'his']), (2, 0, ['up', 'his', 'gardon'])]\n",
      "================ 15 ===================\n",
      "[(0, 0, ['the', 'wind', 'belu']), (1, 0, ['wind', 'belu', 'the']), (2, 0, ['belu', 'the', 'leaves'])]\n",
      "================ 16 ===================\n",
      "[(0, 0, ['Mr', 'J.', 'was']), (1, 89.0, ['J.', 'was', 'very']), (2, 0, ['was', 'very', 'angray'])]\n",
      "================ 17 ===================\n",
      "[(0, 16849.0, ['garden', 'full', 'of']), (1, 0, ['full', 'of', 'leavs'])]\n",
      "================ 18 ===================\n",
      "[(0, 1416811.0, ['talk', 'to', 'the']), (1, 7829.0, ['to', 'the', 'manger'])]\n",
      "================ 19 ===================\n",
      "[(0, 9460.0, ['they', 'throw', 'a']), (1, 0, ['throw', 'a', 'aero'])]\n",
      "================ 20 ===================\n",
      "[(0, 0, ['an', 'ansion', 'method']), (1, 0, ['ansion', 'method', 'of']), (2, 3034.0, ['method', 'of', 'hunting'])]\n",
      "================ 21 ===================\n",
      "[(0, 273.0, ['after', 'the', 'dear'])]\n",
      "================ 22 ===================\n",
      "[(0, 0, ['the', 'birds', 'flu']), (1, 0, ['birds', 'flu', 'up'])]\n",
      "================ 23 ===================\n",
      "[(0, 0, ['making', 'any', 'noice'])]\n",
      "================ 24 ===================\n",
      "[(0, 0, ['bring', 'it', 'stright']), (1, 51.0, ['it', 'stright', 'to']), (2, 1109.0, ['stright', 'to', 'the']), (3, 6236.0, ['to', 'the', 'hunters'])]\n",
      "================ 25 ===================\n",
      "[(0, 0, ['this', 'menes', 'a']), (1, 0, ['menes', 'a', 'man']), (2, 1265.0, ['a', 'man', 'waits'])]\n",
      "================ 26 ===================\n",
      "[(0, 0, ['a', 'man', 'waight']), (1, 0, ['man', 'waight', 'for']), (2, 0, ['waight', 'for', 'the']), (3, 114077.0, ['for', 'the', 'animal'])]\n",
      "================ 27 ===================\n",
      "[(0, 0, ['an', 'arow', 'in']), (1, 0, ['arow', 'in', 'his']), (2, 722681.0, ['in', 'his', 'hand'])]\n",
      "================ 28 ===================\n",
      "[(0, 0, ['they', 'dick', 'a']), (1, 0, ['dick', 'a', 'hole'])]\n",
      "================ 29 ===================\n",
      "[(0, 6262.0, ['a', 'pice', 'of']), (1, 0, ['pice', 'of', 'bait'])]\n",
      "================ 30 ===================\n",
      "[(0, 0, ['this', 'tarpp', 'looks']), (1, 0, ['tarpp', 'looks', 'like']), (2, 2443576.0, ['looks', 'like', 'a']), (3, 6416.0, ['like', 'a', 'cave'])]\n",
      "================ 31 ===================\n",
      "[(0, 0, ['they', 'make', 'nose'])]\n",
      "================ 32 ===================\n",
      "[(0, 136519.0, ['the', 'animals', 'are']), (1, 0, ['animals', 'are', 'skeard'])]\n",
      "================ 33 ===================\n",
      "[(0, 0, ['the', 'ribbetes', 'will']), (1, 0, ['ribbetes', 'will', 'run'])]\n",
      "================ 34 ===================\n",
      "[(0, 0, ['they', 'throw', 'stons'])]\n",
      "================ 35 ===================\n",
      "[(0, 0, ['they', 'chaching', 'the']), (1, 0, ['chaching', 'the', 'animals'])]\n",
      "================ 36 ===================\n",
      "[(0, 0, ['three', 'handred', 'people'])]\n",
      "================ 37 ===================\n",
      "[(0, 0, ['it', 'was', 'coulorful'])]\n",
      "================ 38 ===================\n",
      "[(0, 46072.0, ['I', 'saw', 'many']), (1, 0, ['saw', 'many', 'crowns'])]\n",
      "================ 39 ===================\n",
      "[(0, 498939.0, ['could', 'do', 'it']), (1, 89.0, ['do', 'it', 'easly'])]\n",
      "================ 40 ===================\n",
      "[(0, 0, ['blood', 'was', 'comming']), (1, 924.0, ['was', 'comming', 'out'])]\n",
      "================ 41 ===================\n",
      "[(0, 7332.0, ['you', 'have', 'wondered']), (1, 58.0, ['have', 'wondered', 'away'])]\n",
      "================ 42 ===================\n",
      "[(0, 78.0, ['you', 'can', 'controle']), (1, 0, ['can', 'controle', 'with']), (2, 0, ['controle', 'with', 'your']), (3, 9622.0, ['with', 'your', 'toes'])]\n",
      "================ 43 ===================\n",
      "[(0, 0, ['chating', 'to', 'each']), (1, 4061563.0, ['to', 'each', 'other'])]\n",
      "================ 44 ===================\n",
      "[(0, 1574063.0, ['out', 'from', 'the']), (1, 0, ['from', 'the', 'wardrope'])]\n",
      "================ 45 ===================\n",
      "[(0, 0, ['for', 'controlling', 'temerature'])]\n",
      "================ 46 ===================\n",
      "[(0, 2226.0, ['friendly', 'with', 'your']), (1, 0, ['with', 'your', 'neibours'])]\n",
      "================ 47 ===================\n",
      "[(0, 0, ['the', 'prinsess', 'has']), (1, 0, ['prinsess', 'has', 'given']), (2, 52693.0, ['has', 'given', 'birth'])]\n",
      "================ 48 ===================\n",
      "[(0, 0, ['to', 'make', 'arrengiments'])]\n",
      "================ 49 ===================\n",
      "[(0, 54366.0, ['hit', 'him', 'with']), (1, 721696.0, ['him', 'with', 'a']), (2, 167.0, ['with', 'a', 'hamer'])]\n",
      "================ 50 ===================\n",
      "[(0, 0, ['they', 'mite', 'see']), (1, 77.0, ['mite', 'see', 'you'])]\n",
      "================ 51 ===================\n",
      "[(0, 0, ['transport', 'is', 'deferent'])]\n",
      "================ 52 ===================\n",
      "[(0, 113895.0, ['they', 'have', 'different']), (1, 0, ['have', 'different', 'cloves'])]\n",
      "================ 53 ===================\n",
      "[(0, 0, ['a', 'plastic', 'sharpner'])]\n",
      "================ 54 ===================\n",
      "[(0, 0, ['the', 'chinease', 'invented']), (1, 0, ['chinease', 'invented', 'paper'])]\n",
      "================ 55 ===================\n",
      "[(0, 0, ['a', 'bottal', 'of']), (1, 0, ['bottal', 'of', 'lemonade'])]\n",
      "================ 56 ===================\n",
      "[(0, 6081.0, ['to', 'here', 'you'])]\n",
      "================ 57 ===================\n",
      "[(0, 11541.0, ['on', 'and', 'of'])]\n",
      "================ 58 ===================\n",
      "[(0, 56309540.0, ['if', 'you', 'are']), (1, 0, ['you', 'are', 'thisty'])]\n",
      "================ 59 ===================\n",
      "[(0, 0, ['a', 'march', 'box'])]\n",
      "================ 60 ===================\n",
      "[(0, 1375132.0, ['all', 'of', 'a']), (1, 0, ['of', 'a', 'sarden'])]\n",
      "================ 61 ===================\n",
      "[(0, 96219.0, ['went', 'up', 'the']), (1, 0, ['up', 'the', 'certon'])]\n",
      "================ 62 ===================\n",
      "[(0, 0, ['with', 'babbles', 'in']), (1, 0, ['babbles', 'in', 'it'])]\n",
      "================ 63 ===================\n",
      "[(0, 832595.0, ['so', 'I', 'had']), (1, 7170797.0, ['I', 'had', 'a']), (2, 0, ['had', 'a', 'barth'])]\n",
      "================ 64 ===================\n",
      "[(0, 0, ['I', 'made', 'clouths']), (1, 0, ['made', 'clouths', 'with']), (2, 0, ['clouths', 'with', 'it'])]\n",
      "================ 65 ===================\n",
      "[(0, 0, ['I', 'was', 'skeard']), (1, 0, ['was', 'skeard', 'of']), (2, 0, ['skeard', 'of', 'it'])]\n",
      "================ 66 ===================\n",
      "[(0, 346.0, ['throw', 'the', 'window'])]\n",
      "================ 67 ===================\n",
      "[(0, 7274012.0, ['one', 'of', 'my']), (1, 0, ['of', 'my', 'exsiting']), (2, 0, ['my', 'exsiting', 'days'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ 68 ===================\n",
      "[(0, 0, ['found', 'some', 'crames']), (1, 0, ['some', 'crames', 'and']), (2, 0, ['crames', 'and', 'ate']), (3, 13984.0, ['and', 'ate', 'them'])]\n",
      "================ 69 ===================\n",
      "[(0, 1680631.0, ['I', 'was', 'very']), (1, 0, ['was', 'very', 'carful'])]\n",
      "================ 70 ===================\n",
      "[(0, 117563.0, ['speaking', 'to', 'a']), (1, 0, ['to', 'a', 'gost'])]\n",
      "================ 71 ===================\n",
      "[(0, 0, ['to', 'mak', 'people']), (1, 0, ['mak', 'people', 'scared'])]\n",
      "================ 72 ===================\n",
      "[(0, 41.0, ['lit', 'a', 'candel'])]\n",
      "================ 73 ===================\n",
      "[(0, 182825.0, ['a', 'man', 'or']), (1, 109224.0, ['man', 'or', 'a']), (2, 0, ['or', 'a', 'laddy'])]\n",
      "================ 74 ===================\n",
      "[(0, 0, ['something', 'bard', 'happened'])]\n",
      "================ 75 ===================\n",
      "[(0, 0, ['thander', 'and', 'lightning'])]\n",
      "================ 76 ===================\n",
      "[(0, 3260.0, ['the', 'inspector,', 'the']), (1, 0, ['inspector,', 'the', 'sargent'])]\n",
      "================ 77 ===================\n",
      "[(0, 136921.0, ['a', 'light', 'and']), (1, 67475.0, ['light', 'and', 'a']), (2, 14416.0, ['and', 'a', 'nose'])]\n",
      "================ 78 ===================\n",
      "[(0, 0, ['I', 'would', 'rube']), (1, 0, ['would', 'rube', 'a']), (2, 0, ['rube', 'a', 'rag'])]\n",
      "================ 79 ===================\n",
      "[(0, 0, ['cruch', 'a', 'police']), (1, 89294.0, ['a', 'police', 'car'])]\n",
      "================ 80 ===================\n",
      "[(0, 0, ['Ian', 'triped', 'over'])]\n",
      "================ 81 ===================\n",
      "[(0, 0, ['got', 'a', 'tuisse']), (1, 0, ['a', 'tuisse', 'from']), (2, 0, ['tuisse', 'from', 'her']), (3, 8387.0, ['from', 'her', 'bag'])]\n",
      "================ 82 ===================\n",
      "[(0, 2900.0, ['want', 'do', 'you']), (1, 1580311.0, ['do', 'you', 'mean'])]\n",
      "================ 83 ===================\n",
      "[(0, 1107.0, [\"I'm\", 'writting', 'a']), (1, 532.0, ['writting', 'a', 'letter'])]\n",
      "================ 84 ===================\n",
      "[(0, 124.0, ['a', 'straigt', 'line'])]\n",
      "================ 85 ===================\n",
      "[(0, 0, ['I', 'througt', 'it']), (1, 0, ['througt', 'it', 'was']), (2, 771182.0, ['it', 'was', 'going']), (3, 6734858.0, ['was', 'going', 'to']), (4, 41969.0, ['going', 'to', 'rain'])]\n",
      "================ 86 ===================\n",
      "[(0, 0, ['he', 'just', 'whated']), (1, 0, ['just', 'whated', 'to']), (2, 0, ['whated', 'to', 'die'])]\n",
      "================ 87 ===================\n",
      "[(0, 0, ['bounsing', 'a', 'ball'])]\n",
      "================ 88 ===================\n",
      "[(0, 0, ['ate', 'some', 'fruts'])]\n",
      "================ 89 ===================\n",
      "[(0, 0, ['he', \"doesn't\", 'noteics'])]\n",
      "================ 90 ===================\n",
      "[(0, 0, ['a', 'bull', 'charcing']), (1, 0, ['bull', 'charcing', 'at']), (2, 0, ['charcing', 'at', 'him'])]\n",
      "================ 91 ===================\n",
      "[(0, 0, ['and', 'ice-cream', 'stroe'])]\n",
      "================ 92 ===================\n",
      "[(0, 14884564.0, ['there', 'was', 'a']), (1, 0, ['was', 'a', 'girrife'])]\n",
      "================ 93 ===================\n",
      "[(0, 0, ['then', 'a', 'circodile']), (1, 0, ['a', 'circodile', 'came'])]\n",
      "================ 94 ===================\n",
      "[(0, 0, ['iorn', 'is', 'a']), (1, 392218.0, ['is', 'a', 'hard']), (2, 2748.0, ['a', 'hard', 'metal'])]\n",
      "================ 95 ===================\n",
      "[(0, 0, ['four', 'pen', 'kives'])]\n",
      "================ 96 ===================\n",
      "[(0, 1190626.0, ['there', 'are', 'four']), (1, 0, ['are', 'four', 'chimmys'])]\n",
      "================ 97 ===================\n",
      "[(0, 0, ['on', 'the', 'blackbroad'])]\n",
      "================ 98 ===================\n",
      "[(0, 0, ['on', 'the', 'golb'])]\n",
      "================ 99 ===================\n",
      "[(0, 0, ['in', 'the', 'cupbroad'])]\n",
      "================ 100 ===================\n",
      "[(0, 0, ['the', 'light', 'swith'])]\n",
      "================ 101 ===================\n",
      "[(0, 0, ['from', 'the', 'thinest'])]\n",
      "================ 102 ===================\n",
      "[(0, 0, ['to', 'the', 'fatest'])]\n",
      "================ 103 ===================\n",
      "[(0, 984.0, ['in', 'a', 'feild'])]\n",
      "================ 104 ===================\n",
      "[(0, 0, ['a', 'cerry', 'red']), (1, 0, ['cerry', 'red', 'dress'])]\n",
      "================ 105 ===================\n",
      "[(0, 50047.0, ['the', 'being', 'of']), (1, 579879.0, ['being', 'of', 'the']), (2, 498571.0, ['of', 'the', 'play'])]\n",
      "================ 106 ===================\n",
      "[(0, 10444.0, ['fried', 'rice', 'and']), (1, 0, ['rice', 'and', 'nodles'])]\n",
      "================ 107 ===================\n",
      "[(0, 0, ['carry', 'money', 'whith']), (1, 0, ['money', 'whith', 'me'])]\n",
      "================ 108 ===================\n",
      "[(0, 0, ['they', 'are', 'belive']), (1, 0, ['are', 'belive', 'superstitious'])]\n",
      "================ 109 ===================\n",
      "[(0, 1184386.0, ['the', 'things', 'I']), (1, 70.0, ['things', 'I', 'belive'])]\n",
      "================ 110 ===================\n",
      "[(0, 20547.0, ['differences', 'between', 'their']), (1, 0, ['between', 'their', 'knowledg'])]\n",
      "================ 111 ===================\n",
      "[(0, 0, ['by', 'logical', 'speach'])]\n",
      "================ 112 ===================\n",
      "[(0, 0, ['a', 'white', 'coulor'])]\n",
      "================ 113 ===================\n",
      "[(0, 0, [\"don't\", 'make', 'anything']), (1, 0, ['make', 'anything', 'beark'])]\n",
      "================ 114 ===================\n",
      "[(0, 0, ['to', 'feind', 'out'])]\n",
      "================ 115 ===================\n",
      "[(0, 0, ['when', 'a', 'cuple'])]\n",
      "================ 116 ===================\n",
      "[(0, 0, ['the', 'dustpin', 'is']), (1, 0, ['dustpin', 'is', 'being']), (2, 0, ['is', 'being', 'emptyed'])]\n",
      "================ 117 ===================\n",
      "[(0, 0, ['the', 'dustpin', 'is']), (1, 0, ['dustpin', 'is', 'being']), (2, 0, ['is', 'being', 'emptyed'])]\n",
      "================ 118 ===================\n",
      "[(0, 104918.0, ['the', 'roof', 'is']), (1, 2303.0, ['roof', 'is', 'being']), (2, 0, ['is', 'being', 'repared'])]\n",
      "================ 119 ===================\n",
      "[(0, 15511927.0, ['have', 'to', 'be']), (1, 0, ['to', 'be', 'emtied']), (2, 0, ['be', 'emtied', 'every']), (3, 0, ['emtied', 'every', 'day'])]\n",
      "================ 120 ===================\n",
      "[(0, 0, ['they', 'where', 'cleaned'])]\n",
      "================ 121 ===================\n",
      "[(0, 45.0, ['he', 'belive', 'in']), (1, 6020.0, ['belive', 'in', 'the']), (2, 2232149.0, ['in', 'the', 'old']), (3, 34728.0, ['the', 'old', 'story'])]\n",
      "================ 122 ===================\n",
      "[(0, 240.0, ['the', 'most', 'fameous']), (1, 0, ['most', 'fameous', 'player'])]\n",
      "================ 123 ===================\n",
      "[(0, 72.0, ['of', 'the', 'centery'])]\n",
      "================ 124 ===================\n",
      "[(0, 166351.0, ['the', 'man', 'had']), (1, 0, ['man', 'had', 'tired']), (2, 241.0, ['had', 'tired', 'to']), (3, 0, ['tired', 'to', 'smuggle'])]\n",
      "================ 125 ===================\n",
      "[(0, 0, ['put', 'more', 'petroil'])]\n",
      "================ 126 ===================\n",
      "[(0, 1534.0, ['he', 'tryed', 'to'])]\n",
      "================ 127 ===================\n",
      "[(0, 0, ['heard', 'a', 'extrange']), (1, 0, ['a', 'extrange', 'noise'])]\n",
      "================ 128 ===================\n",
      "[(0, 1534.0, ['he', 'tryed', 'to'])]\n",
      "================ 129 ===================\n",
      "[(0, 0, ['he', 'hited', 'him'])]\n",
      "================ 130 ===================\n",
      "[(0, 0, [\"he'd\", 'forgoten', 'to'])]\n",
      "================ 131 ===================\n",
      "[(0, 0, [\"he'd\", 'setted', 'fire']), (1, 0, ['setted', 'fire', 'to'])]\n",
      "================ 132 ===================\n",
      "[(0, 0, [\"he'd\", 'shooted', 'his']), (1, 0, ['shooted', 'his', 'wife'])]\n",
      "================ 133 ===================\n",
      "[(0, 123001.0, ['sent', 'them', 'to']), (1, 0, ['them', 'to', 'prision'])]\n",
      "================ 134 ===================\n",
      "[(0, 0, ['craying', 'her', 'eyes']), (1, 11570.0, ['her', 'eyes', 'out'])]\n",
      "================ 135 ===================\n",
      "[(0, 0, ['las', 'Monday'])]\n",
      "================ 136 ===================\n",
      "[(0, 0, ['put', 'on', 'weigh'])]\n",
      "================ 137 ===================\n",
      "[(0, 2052.0, ['what', 'had', 'happend'])]\n",
      "================ 138 ===================\n",
      "[(0, 0, [\"he'd\", 'safed', 'the']), (1, 0, ['safed', 'the', 'woman'])]\n",
      "================ 139 ===================\n",
      "[(0, 0, ['becom', 'too', 'fat'])]\n",
      "================ 140 ===================\n",
      "[(0, 2052.0, ['what', 'had', 'happend'])]\n",
      "================ 141 ===================\n",
      "[(0, 0, ['in', 'hollidays', 'today'])]\n",
      "================ 142 ===================\n",
      "[(0, 0, ['photographes', 'of', 'footprints'])]\n",
      "================ 143 ===================\n",
      "[(0, 0, ['wich', 'cannot', 'be']), (1, 112.0, ['cannot', 'be', 'explained'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ 144 ===================\n",
      "[(0, 0, [\"it's\", 'a', 'wolfe'])]\n",
      "================ 145 ===================\n",
      "[(0, 0, ['they', \"aren't\", 'discoverd'])]\n",
      "================ 146 ===================\n",
      "[(0, 0, ['to', 'photographe', 'these']), (1, 0, ['photographe', 'these', 'things'])]\n",
      "================ 147 ===================\n",
      "[(0, 0, ['an', 'unborne', 'child'])]\n",
      "================ 148 ===================\n",
      "[(0, 0, ['the', 'misterious', 'beach'])]\n",
      "================ 149 ===================\n",
      "[(0, 1515.0, ['chosing', 'between'])]\n",
      "================ 150 ===================\n",
      "[(0, 0, ['it', 'rapresents', 'the'])]\n",
      "================ 151 ===================\n",
      "[(0, 44.0, ['it', 'is', 'ridicolous']), (1, 0, ['is', 'ridicolous', 'that'])]\n",
      "================ 152 ===================\n",
      "[(0, 0, ['is', 'scientificly', 'proved'])]\n",
      "================ 153 ===================\n",
      "[(0, 100.0, ['there', 'are', 'advertisments']), (1, 49.0, ['are', 'advertisments', 'for'])]\n",
      "================ 154 ===================\n",
      "[(0, 1148890.0, ['it', 'was', 'my']), (1, 0, ['was', 'my', 'immagination'])]\n",
      "================ 155 ===================\n",
      "[(0, 0, ['to', 'bann', 'smoking'])]\n",
      "================ 156 ===================\n",
      "[(0, 0, ['my', 'happyness', 'depends']), (1, 0, ['happyness', 'depends', 'on'])]\n",
      "================ 157 ===================\n",
      "[(0, 0, ['the', 'pleasent', 'smell']), (1, 57.0, ['pleasent', 'smell', 'of'])]\n",
      "================ 158 ===================\n",
      "[(0, 6550.0, ['dialing', 'the', 'number'])]\n",
      "================ 159 ===================\n",
      "[(0, 55.0, ['actualy', 'I', 'had']), (1, 2010571.0, ['I', 'had', 'no'])]\n",
      "================ 160 ===================\n",
      "[(0, 0, ['the', 'most', 'tireing']), (1, 0, ['most', 'tireing', 'part'])]\n",
      "================ 161 ===================\n",
      "[(0, 0, ['gamblers', 'play', 'pocker'])]\n",
      "================ 162 ===================\n",
      "[(0, 88025.0, ['the', 'existance', 'of'])]\n",
      "================ 163 ===================\n",
      "[(0, 0, ['phisical', 'sensations'])]\n",
      "================ 164 ===================\n",
      "[(0, 99739.0, ['it', 'is', 'without']), (1, 0, ['is', 'without', 'doupt'])]\n",
      "================ 165 ===================\n",
      "[(0, 45.0, ['let', 'us', 'analize'])]\n",
      "================ 166 ===================\n",
      "[(0, 0, ['devlopping', 'countries'])]\n",
      "================ 167 ===================\n",
      "[(0, 124.0, ['I', 'personnaly', 'believe'])]\n",
      "================ 168 ===================\n",
      "[(0, 1130020.0, ['as', 'often', 'as']), (1, 127196.0, ['often', 'as', 'I']), (2, 1209.0, ['as', 'I', 'breath'])]\n",
      "================ 169 ===================\n",
      "[(0, 0, ['flowers,', 'perfums'])]\n",
      "================ 170 ===================\n",
      "[(0, 76.0, ['to', 'interprete', 'what']), (1, 0, ['interprete', 'what', \"I've\"]), (2, 49816.0, ['what', \"I've\", 'said'])]\n",
      "================ 171 ===================\n",
      "[(0, 141.0, ['my', 'stomache', 'hurt'])]\n",
      "================ 172 ===================\n",
      "[(0, 0, ['surprised', 'and', 'paralised']), (1, 0, ['and', 'paralised', 'by'])]\n",
      "================ 173 ===================\n",
      "[(0, 173.0, ['I', 'belief', 'it']), (1, 2038.0, ['belief', 'it', 'was'])]\n",
      "================ 174 ===================\n",
      "[(0, 2515.0, ['when', 'it', 'occured'])]\n",
      "================ 175 ===================\n",
      "[(0, 13453.0, ['she', 'asked', 'us']), (1, 0, ['asked', 'us', 'wether'])]\n",
      "================ 176 ===================\n",
      "[(0, 63.0, ['an', 'extremly', 'hot']), (1, 0, ['extremly', 'hot', 'day'])]\n",
      "================ 177 ===================\n",
      "[(0, 52502845.0, ['be', 'able', 'to']), (1, 6644.0, ['able', 'to', 'chose'])]\n",
      "================ 178 ===================\n",
      "[(0, 74.0, ['their', 'oppinion', 'about'])]\n",
      "================ 179 ===================\n",
      "[(0, 125.0, ['to', 'think', 'wat'])]\n",
      "================ 180 ===================\n",
      "[(0, 0, ['to', 'contenue', 'with'])]\n",
      "================ 181 ===================\n",
      "[(0, 47155.0, ['it', 'was', 'quiet']), (1, 0, ['was', 'quiet', 'successful'])]\n",
      "================ 182 ===================\n",
      "[(0, 0, ['I', 'never', 'thout']), (1, 0, ['never', 'thout', 'that'])]\n",
      "================ 183 ===================\n",
      "[(0, 0, [\"I'd\", 'heart', 'that'])]\n",
      "================ 184 ===================\n",
      "[(0, 0, ['he', 'obvisicly', 'reckoned'])]\n",
      "================ 185 ===================\n",
      "[(0, 0, ['to', 'prays', 'him'])]\n",
      "================ 186 ===================\n",
      "[(0, 0, ['screeming', 'louder', 'than'])]\n",
      "================ 187 ===================\n",
      "[(0, 0, ['to', 'absorbe', 'it'])]\n",
      "================ 188 ===================\n",
      "[(0, 0, ['I', \"couldn't\", 'stop']), (1, 0, [\"couldn't\", 'stop', 'starring'])]\n",
      "================ 189 ===================\n",
      "[(0, 0, ['a', 'street', 'caffe'])]\n",
      "================ 190 ===================\n",
      "[(0, 14233.0, ['a', 'movie', 'was']), (1, 0, ['movie', 'was', 'showen'])]\n",
      "================ 191 ===================\n",
      "[(0, 0, ['companys', 'started', 'to']), (1, 112095.0, ['started', 'to', 'work'])]\n",
      "================ 192 ===================\n",
      "[(0, 264446.0, ['their', 'is'])]\n",
      "================ 193 ===================\n",
      "[(0, 30269.0, ['a', 'tendancy', 'to'])]\n",
      "================ 194 ===================\n",
      "[(0, 342838.0, ['meet', 'in', 'the']), (1, 0, ['in', 'the', 'snak-bar'])]\n",
      "================ 195 ===================\n",
      "[(0, 0, ['security', 'controll'])]\n",
      "================ 196 ===================\n",
      "[(0, 0, ['in', 'condact', 'with'])]\n"
     ]
    }
   ],
   "source": [
    "################### BONUS ######################\n",
    "cor, hits = 0, 0\n",
    "lines = open('lab4.test.1.txt', 'r', encoding='utf8').readlines()\n",
    "result = open('result_bonus.txt', 'w', encoding='utf8')\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "# for line in lines:\n",
    "    print(\"================\", i+1, \"===================\")\n",
    "    print(\"================\", i+1, \"===================\", file=result)\n",
    "    wrong, right = line.split('\\t')\n",
    "\n",
    "    tokens = wrong.strip().split(' ') # words(open('big.txt').read())) # or using regex\n",
    "    lowest_pair, lowest_pos = get_lowest_tri(tokens)\n",
    "    \n",
    "    sent, error_word, right_word, candidates, _ = get_max_sent(tokens, lowest_pos)\n",
    "    sent, wrong, right = ' '.join(sent).strip(), wrong.strip(), right.strip()\n",
    "    \n",
    "    if sent == right: hits += 1\n",
    "    cor += 1\n",
    "    \n",
    "    print(\"Error:\", error_word, file=result)\n",
    "    print(\"Candidates:\", candidates, file=result)\n",
    "    print(\"Correction:\", right_word, file=result)\n",
    "    print(wrong, \"->\", sent, \"(correct:\", right, \")\", file=result)\n",
    "    print(\"hits =\", hits, file=result)\n",
    "    print('\\n', file=result)\n",
    "\n",
    "print(\"Precision:\", hits/cor, file=result)\n",
    "print(\"FalseAlarm:\", (cor-hits)/cor, file=result)\n",
    "\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ 1 ===================\n",
      "================ 2 ===================\n",
      "================ 3 ===================\n",
      "================ 4 ===================\n",
      "================ 5 ===================\n",
      "================ 6 ===================\n",
      "================ 7 ===================\n",
      "================ 8 ===================\n",
      "================ 9 ===================\n",
      "================ 10 ===================\n",
      "================ 11 ===================\n",
      "================ 12 ===================\n",
      "================ 13 ===================\n",
      "================ 14 ===================\n",
      "================ 15 ===================\n",
      "================ 16 ===================\n",
      "================ 17 ===================\n",
      "================ 18 ===================\n",
      "================ 19 ===================\n",
      "================ 20 ===================\n",
      "================ 21 ===================\n",
      "================ 22 ===================\n",
      "================ 23 ===================\n",
      "================ 24 ===================\n",
      "================ 25 ===================\n",
      "================ 26 ===================\n",
      "================ 27 ===================\n",
      "================ 28 ===================\n",
      "================ 29 ===================\n",
      "================ 30 ===================\n",
      "================ 31 ===================\n",
      "================ 32 ===================\n",
      "================ 33 ===================\n",
      "================ 34 ===================\n",
      "================ 35 ===================\n",
      "================ 36 ===================\n",
      "================ 37 ===================\n",
      "================ 38 ===================\n",
      "================ 39 ===================\n",
      "================ 40 ===================\n",
      "================ 41 ===================\n",
      "================ 42 ===================\n",
      "================ 43 ===================\n",
      "================ 44 ===================\n",
      "================ 45 ===================\n",
      "================ 46 ===================\n",
      "================ 47 ===================\n",
      "================ 48 ===================\n",
      "================ 49 ===================\n",
      "================ 50 ===================\n",
      "================ 51 ===================\n",
      "================ 52 ===================\n",
      "================ 53 ===================\n",
      "================ 54 ===================\n",
      "================ 55 ===================\n",
      "================ 56 ===================\n",
      "================ 57 ===================\n",
      "================ 58 ===================\n",
      "================ 59 ===================\n",
      "================ 60 ===================\n",
      "================ 61 ===================\n",
      "================ 62 ===================\n",
      "================ 63 ===================\n",
      "================ 64 ===================\n",
      "================ 65 ===================\n",
      "================ 66 ===================\n",
      "================ 67 ===================\n",
      "================ 68 ===================\n",
      "================ 69 ===================\n",
      "================ 70 ===================\n",
      "================ 71 ===================\n",
      "================ 72 ===================\n",
      "================ 73 ===================\n",
      "================ 74 ===================\n",
      "================ 75 ===================\n",
      "================ 76 ===================\n",
      "================ 77 ===================\n",
      "================ 78 ===================\n",
      "================ 79 ===================\n",
      "================ 80 ===================\n",
      "================ 81 ===================\n",
      "================ 82 ===================\n",
      "================ 83 ===================\n",
      "================ 84 ===================\n",
      "================ 85 ===================\n",
      "================ 86 ===================\n",
      "================ 87 ===================\n",
      "================ 88 ===================\n",
      "================ 89 ===================\n",
      "================ 90 ===================\n",
      "================ 91 ===================\n",
      "================ 92 ===================\n",
      "================ 93 ===================\n",
      "================ 94 ===================\n",
      "================ 95 ===================\n",
      "================ 96 ===================\n",
      "================ 97 ===================\n",
      "================ 98 ===================\n",
      "================ 99 ===================\n",
      "================ 100 ===================\n",
      "================ 101 ===================\n",
      "================ 102 ===================\n",
      "================ 103 ===================\n",
      "================ 104 ===================\n",
      "================ 105 ===================\n",
      "================ 106 ===================\n",
      "================ 107 ===================\n",
      "================ 108 ===================\n",
      "================ 109 ===================\n",
      "================ 110 ===================\n",
      "================ 111 ===================\n",
      "================ 112 ===================\n",
      "================ 113 ===================\n",
      "================ 114 ===================\n",
      "================ 115 ===================\n",
      "================ 116 ===================\n",
      "================ 117 ===================\n",
      "================ 118 ===================\n",
      "================ 119 ===================\n",
      "================ 120 ===================\n",
      "================ 121 ===================\n",
      "================ 122 ===================\n",
      "================ 123 ===================\n",
      "================ 124 ===================\n",
      "================ 125 ===================\n",
      "================ 126 ===================\n",
      "================ 127 ===================\n",
      "================ 128 ===================\n",
      "================ 129 ===================\n",
      "================ 130 ===================\n",
      "================ 131 ===================\n",
      "================ 132 ===================\n",
      "================ 133 ===================\n",
      "================ 134 ===================\n",
      "================ 135 ===================\n",
      "================ 136 ===================\n",
      "================ 137 ===================\n",
      "================ 138 ===================\n",
      "================ 139 ===================\n",
      "================ 140 ===================\n",
      "================ 141 ===================\n",
      "================ 142 ===================\n",
      "================ 143 ===================\n",
      "================ 144 ===================\n",
      "================ 145 ===================\n",
      "================ 146 ===================\n",
      "================ 147 ===================\n",
      "================ 148 ===================\n",
      "================ 149 ===================\n",
      "================ 150 ===================\n",
      "================ 151 ===================\n",
      "================ 152 ===================\n",
      "================ 153 ===================\n",
      "================ 154 ===================\n",
      "================ 155 ===================\n",
      "================ 156 ===================\n",
      "================ 157 ===================\n",
      "================ 158 ===================\n",
      "================ 159 ===================\n",
      "================ 160 ===================\n",
      "================ 161 ===================\n",
      "================ 162 ===================\n",
      "================ 163 ===================\n",
      "================ 164 ===================\n",
      "================ 165 ===================\n",
      "================ 166 ===================\n",
      "================ 167 ===================\n",
      "================ 168 ===================\n",
      "================ 169 ===================\n",
      "================ 170 ===================\n",
      "================ 171 ===================\n",
      "================ 172 ===================\n",
      "================ 173 ===================\n",
      "================ 174 ===================\n",
      "================ 175 ===================\n",
      "================ 176 ===================\n",
      "================ 177 ===================\n",
      "================ 178 ===================\n",
      "================ 179 ===================\n",
      "================ 180 ===================\n",
      "================ 181 ===================\n",
      "================ 182 ===================\n",
      "================ 183 ===================\n",
      "================ 184 ===================\n",
      "================ 185 ===================\n",
      "================ 186 ===================\n",
      "================ 187 ===================\n",
      "================ 188 ===================\n",
      "================ 189 ===================\n",
      "================ 190 ===================\n",
      "================ 191 ===================\n",
      "================ 192 ===================\n",
      "================ 193 ===================\n",
      "================ 194 ===================\n",
      "================ 195 ===================\n",
      "================ 196 ===================\n"
     ]
    }
   ],
   "source": [
    "################### BONUS2 ######################\n",
    "cor, hits = 0, 0\n",
    "lines = open('lab4.test.2.txt', 'r', encoding='utf8').readlines()\n",
    "result = open('result_bonus2.txt', 'w', encoding='utf8')\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "# for line in lines:\n",
    "    print(\"================\", i+1, \"===================\")\n",
    "    print(\"================\", i+1, \"===================\", file=result)\n",
    "    wrong, right = line.split('\\t')\n",
    "\n",
    "    tokens = wrong.strip().split(' ') # words(open('big.txt').read())) # or using regex\n",
    "    lowest_pair, lowest_pos = get_lowest_tri(tokens)\n",
    "    \n",
    "    sent, error_word, right_word, candidates, _ = get_max_sent(tokens, lowest_pos)\n",
    "    sent, wrong, right = ' '.join(sent).strip(), wrong.strip(), right.strip()\n",
    "    \n",
    "    if sent == right: hits += 1\n",
    "    cor += 1\n",
    "    \n",
    "    print(\"Error:\", error_word, file=result)\n",
    "    print(\"Candidates:\", candidates, file=result)\n",
    "    print(\"Correction:\", right_word, file=result)\n",
    "    print(wrong, \"->\", sent, \"(correct:\", right, \")\", file=result)\n",
    "    print(\"hits =\", hits, file=result)\n",
    "    print('\\n', file=result)\n",
    "\n",
    "print(\"Precision:\", hits/cor, file=result)\n",
    "print(\"FalseAlarm:\", (cor-hits)/cor, file=result)\n",
    "\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
